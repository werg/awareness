# LLMs with Awareness: Decoupled Contextual Memory

## 1. Abstract

Standard Transformer architectures are strictly bound by the context window, where computational complexity scales quadratically $O(N^2)$ and KV-cache requirements scale linearly. This binds "reasoning capacity" directly to "immediate context availability," preventing efficient handling of massive, mutable corpora like code repositories or knowledge bases.

**Project Awareness** decouples **Reasoning** (Decoder) from **Context** (Encoder). Instead of forcing raw tokens into the prompt, we project a mutable corpus into a latent Key/Value (KV) store. The primary LLM attends to this store via Cross-Attention layers, allowing it to access massive contexts without re-computation, enabling repository-scale awareness with constant-time inference cost per token generated.

---

## 2. Architectural Specification

The system comprises three strictly defined components: the **Context Encoder**, the **Latent Memory Store**, and the **Reasoning Kernel (Decoder)**.

### 2.1 The Context Encoder ($E_\theta$)
A lightweight, bidirectional Transformer optimized solely for representation, not generation.

*   **Input:** Discrete document chunks (files, diffs, wiki articles).
*   **Function:** Maps raw tokens $X$ to latent memory representations $(K_{mem}, V_{mem})$. Unlike standard embeddings, these are explicitly shaped to be consumed by attention heads.
*   **Operational invariant:** $E_\theta$ is run *asynchronously*. When a document $d_i$ is modified, only $E_\theta(d_i)$ is re-computed. The global context is never fully re-processed.
*   **Output:** A compressed sequence of KV tensors, distinct from the decoder's internal states.

### 2.2 The Latent Memory Store ($\mathcal{M}$)
A structured tensor database holding the output of $E_\theta$.

*   **Structure:** A persistent map $\{id_i \rightarrow (K_i, V_i)\}$.
*   **Retrieval:** Instead of retrieving text (RAG), the system retrieves *pre-computed attention tensors*.
*   **Granularity:** Supports arbitrary retrieval scopes (e.g., "All files in `/src`", "User Profile", "Recent Interaction History").

### 2.3 The Reasoning Kernel / Decoder ($D_\phi$)
A dense decoder-only LLM (e.g., 8Bâ€“14B parameter range) augmented with **Gated Cross-Attention (GCA)** blocks.

*   **Base Architecture:** Standard Causal Self-Attention (CSA) handles the immediate instruction and local scratchpad.
*   **Augmentation:** In the upper $\frac{1}{3}$ of the network, CSA blocks are interleaved with GCA blocks.
*   **Mechanism:**
    $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q_{loc} K_{mem}^T}{\sqrt{d_k}}\right) V_{mem} $$
    *   $Q_{loc}$: Queries generated by the Decoder from the current prompt.
    *   $K_{mem}, V_{mem}$: Pre-computed tensors fetched from $\mathcal{M}$.
*   **Gradient Flow:** During training, $\nabla$ propagates from $D_\phi$ through the cross-attention mechanism into $E_\theta$, forcing the encoder to learn representations that are semantically useful for the decoder's reasoning tasks.

---

## 3. Training Methodology: Contextual Distillation

For initial training, we use **Teacher-Student Distillation**.

### 3.1 The Objective
The **Student** (Awareness Model) must replicate the outputs of a **Teacher** (Long-Context SOTA Model), but without seeing the Teacher's context window.

*   **Teacher:** Receives `[Instruction + Full Repository Dump]`.
*   **Student:** Receives `[Instruction]` + `[Latent Memory of Repository]`.

### 3.2 The Loss Landscape
1.  **KL Divergence / Cross-Entropy:** The Student must match the Teacher's token distribution.
2.  **Sparsity Regularization (Optional):** Penalize the encoder for producing redundant KV pairs, encouraging semantic compression.
3.  **Auxiliary Task - Citation:** The Student is trained to predict *which* document $d_i$ contributed to the answer, enforcing grounding and reducing hallucination.

### 3.3 Training Curriculum
1.  **Phase I: Repository Reconstruction:** Masked code completion where the "mask" requires information from a different file in the repo.
2.  **Phase II: Instruction Following:** Multi-hop reasoning tasks ("Refactor class X based on the interface defined in file Y").
3.  **Phase III: State Tracking:** Tasks involving mutable variables across a user session, requiring the model to read/write to memory conceptually.

---

## 4. Target Application: The "Infinite" Coding Agent

The primary implementation target is a coding agent with **Full-Repo Awareness**.

### 4.1 The Bottleneck
Current coding assistants rely on RAG (imprecise) or stuffing context (expensive/limited). They fail at:
*   System-wide refactors.
*   Understanding distant dependencies.
*   Maintaining awareness of project structure updates.

### 4.2 The Awareness Solution
*   **Initialization:** The entire repository is passed through the Context Encoder once.
*   **Inference:** The user asks to "fix the bug in `auth.py`." The Decoder attends to `auth.py` (local context) and Cross-Attends to the entire repo's KV store (latent memory).
*   **Mutation:** The agent writes code. The modified file is re-encoded in milliseconds. The memory store is updated. The agent immediately "sees" the change in its latent space.

---

## 5. Implementation Plan

### 5.1 Model Selection
*   **Decoder ($D_\phi$):** Likely models from the Qwen3 series, prototype with a smaller Qwen3 model, then scale up to Qwen3 coder. Alternatively GPT-OSS.
*   **Encoder ($E_\theta$):** Qwen3 Embedding model.

### 5.2 Dataset Ideas

1.  **The "RepoStack":** GitHub repositories.
2.  **Distilled Traces:**
    *   Take existing commits and reverse-encode them as tasks prompts using an LLM.
    *   Distill larger Qwen3 model teacher creating the commits.
3.  **Negative Sampling:** Training the model *not* to attend to irrelevant files (anti-hallucination).

### 5.3 Evaluation Metrics
*   **Perplexity (PPL) on Remote Context:** Can the model predict code in File A that depends on a definition in File B?
*   **Needle-in-a-Haystack (Repo Edition):** Retrieval accuracy of specific function definitions amidst 100k lines of code.

---

## 6. Significance

**Awareness** represents a shift from **stateless** to **stateful** LLM architectures. By treating context as a database of pre-computed tensors rather than a sequence of raw tokens, we achieve:

1.  **Context Decoupling:** Context size is limited only by disk/RAM, not VRAM or $O(N^2)$ attention walls.
2.  **Persistence:** Knowledge bases do not need to be re-digested for every query.
3.  **Privacy/Security:** Memory stores can be physically separated or permission-gated at the tensor retrieval level without retraining the core model.
