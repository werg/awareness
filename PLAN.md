# LLMs with Awareness: Decoupled Contextual Memory

## 1. Abstract

Standard Transformer architectures are strictly bound by the context window, where computational complexity scales quadratically $O(N^2)$ and KV-cache requirements scale linearly. This binds "reasoning capacity" directly to "immediate context availability," preventing efficient handling of massive, mutable corpora like code repositories or knowledge bases.

**Project Awareness** decouples **Reasoning** (Decoder) from **Context** (Encoder). Instead of forcing raw tokens into the prompt, we project a mutable corpus into a latent Key/Value (KV) store. The primary LLM attends to this store via Cross-Attention layers, allowing it to access massive contexts without re-computation, enabling repository-scale awareness with constant-time inference cost per token generated.

---

## 2. Architectural Specification

The system comprises three strictly defined components: the **Context Encoder**, the **Latent Memory Store**, and the **Reasoning Kernel (Decoder)**.

### 2.1 The Context Encoder ($E_\theta$)
A lightweight, bidirectional Transformer optimized solely for representation, not generation.

*   **Input:** Discrete document chunks (files, diffs, wiki articles).
*   **Function:** Maps raw tokens $X$ to latent memory representations $(K_{mem}, V_{mem})$. Unlike standard embeddings, these are explicitly shaped to be consumed by attention heads.
*   **Operational invariant:** $E_\theta$ is run *asynchronously*. When a document $d_i$ is modified, only $E_\theta(d_i)$ is re-computed. The global context is never fully re-processed.
*   **Output:** A compressed sequence of KV tensors, distinct from the decoder's internal states.

### 2.2 The Latent Memory Store ($\mathcal{M}$)
A structured tensor database holding the output of $E_\theta$.

*   **Structure:** A persistent map $\{id_i \rightarrow (K_i, V_i)\}$.
*   **Retrieval:** Instead of retrieving text (RAG), the system retrieves *pre-computed attention tensors*.
*   **Granularity:** Supports arbitrary retrieval scopes (e.g., "All files in `/src`", "User Profile", "Recent Interaction History").

### 2.3 The Reasoning Kernel / Decoder ($D_\phi$)
A dense decoder-only LLM (e.g., 8B–14B parameter range) augmented with **Gated Cross-Attention (GCA)** blocks.

*   **Base Architecture:** Standard Causal Self-Attention (CSA) handles the immediate instruction and local scratchpad.
*   **Augmentation:** Every 3rd decoder layer receives a GCA block, starting from ~$\frac{1}{3}$ into the network (RETRO-style sparse placement).
*   **Mechanism:**
    $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q_{loc} K_{mem}^T}{\sqrt{d_k}}\right) V_{mem} $$
    *   $Q_{loc}$: Queries generated by the Decoder from the current prompt.
    *   $K_{mem}, V_{mem}$: Pre-computed tensors fetched from $\mathcal{M}$.
*   **Gradient Flow:** During training, $\nabla$ propagates from $D_\phi$ through the cross-attention mechanism into $E_\theta$, forcing the encoder to learn representations that are semantically useful for the decoder's reasoning tasks.

---

## 3. Training Methodology: Staged Contextual Learning

Training proceeds through progressive stages, each building on the previous. All stages use an **agentic frame**—the model always simulates reading files before editing, mirroring real-world tool use patterns. This is critical because the awareness context lacks positional encoding; the model must learn to ground itself by explicitly reading target files.

### 3.1 Stage 0: Context Grounding (Pre-Coding)

**Objective:** Teach the decoder to *use* cross-attention over encoded context before introducing code generation complexity.

**Freeze schedule:** Phase A (decoder frozen) → Phase B (decoder unfrozen at lr=1e-6) once GCA gates stabilize above ~0.4. See §3.6.1.

#### 3.1.1 Codebase Q&A

*   **Data Source:** Repositories with auto-generated questions.
*   **Question Types:**
    *   **Structural:** "What files import `utils.py`?", "List all classes in the `/models` directory."
    *   **Factual:** "What is the return type of `parse_config()`?", "What database does this project use?"
    *   **Relational:** "Which function calls `authenticate()` and also accesses `user.permissions`?"
*   **Generation:** Use AST parsing, dependency graphs, and simple templates to generate Q&A pairs automatically. No LLM needed for question generation.
*   **Agentic Frame:** Model must emit `<read file="...">` tokens before answering, training the read-before-act pattern.

#### 3.1.2 Document Corpus Q&A

*   **Data Source:** Wikipedia, technical documentation, textbooks—any large text corpus.
*   **Setup:** Encode corpus sections into latent memory. Ask questions answerable only by cross-attending to specific sections.
*   **Benefit:** Decouples "learning to use cross-attention" from "learning to code," allowing faster iteration on the attention mechanism.

#### 3.1.3 Needle-in-Haystack Variants

*   Embed specific facts in large encoded contexts.
*   Train retrieval precision: model must locate and cite the correct source chunk.
*   Gradually increase context size and distractor density.

---

### 3.2 Stage 1: Simple Commit Reproduction

**Objective:** Learn code transformation patterns from real commit data with minimal prompt engineering.

**Freeze schedule:** Phase B continues (decoder unfrozen at lr=1e-6). See §3.6.1.

#### 3.2.1 Data Preparation

*   **Source:** Git history from curated repositories.
*   **Filter:** Single-file commits, pure additions/modifications (no renames, no binary files).
*   **Context:** Encode the pre-commit repository state into latent memory.

#### 3.2.2 Prompt Variants

Train on multiple prompt formulations to avoid overfitting to a single template:

1.  **Raw Commit Message:**
    ```
    User: <commit_message>
    Agent: <read file="path/to/file.py">
    [file contents shown]
    Agent: <edit file="path/to/file.py">
    [predicted diff or new content]
    ```

2.  **Commit Message + File Path Hint:**
    ```
    User: In `path/to/file.py`: <commit_message>
    ```

3.  **Imperative Reformulation:**
    ```
    User: Modify `file.py` to <action derived from commit message>
    ```

#### 3.2.3 Training Signal

*   **Target:** The actual commit diff (or post-commit file state).
*   **Loss:** Standard cross-entropy on the edit output, conditioned on the agentic read.

---

### 3.3 Stage 2: Synthetic Planning Conversations

**Objective:** Improve prompt quality by having an LLM analyze commits and generate realistic user-agent planning dialogues.

**Freeze schedule:** Phase C (all components unfrozen, decoder lr raised to 5e-6). See §3.6.1.

#### 3.3.1 Commit Analysis Pipeline

For each commit:
1.  **Diff Analysis Agent:** Examines the diff and summarizes *what* changed and *why* (inferring intent from code patterns).
2.  **Prompt Synthesis Agent:** Generates a plausible user request that would lead to this commit.
3.  **Planning Dialogue Agent:** Simulates a multi-turn conversation:
    *   User states high-level goal
    *   Agent asks clarifying questions
    *   User provides constraints/preferences
    *   Agent proposes approach
    *   User approves

#### 3.3.2 Example Synthetic Dialogue

```
User: The login endpoint is too slow. Can you optimize it?
Agent: I'll investigate. Let me check the current implementation.
<read file="auth/login.py">
[file contents]
Agent: I see the issue—we're hitting the database twice for user lookup.
       I can consolidate this into a single query. Should I also add caching?
User: Just the single query for now, keep it simple.
Agent: Understood.
<edit file="auth/login.py">
[optimized code]
```

#### 3.3.3 Benefits

*   Richer training signal than raw commit messages (which are often terse or cryptic).
*   Model learns to handle ambiguity, ask questions, and propose plans.
*   Closer to real agentic deployment scenarios.

---

### 3.4 Stage 3: Agent-Improved Training Data

**Objective:** Use an LLM to improve or refactor commit code before using it as training targets.

#### 3.4.1 Code Quality Pipeline

1.  **Style Normalization:** Reformat commits to consistent style (removes noise from style variations).
2.  **Bug Injection/Repair:** Intentionally introduce bugs, train model to fix them.
3.  **Refactoring:** Take "messy" commits and produce cleaner versions as targets.
4.  **Documentation:** Add missing docstrings/comments to target code.

#### 3.4.2 Curriculum Design

*   **Easy → Hard:** Start with style-normalized commits, progress to refactored versions.
*   **Negative Examples:** Include commits that *shouldn't* be made (security vulnerabilities, regressions) with "reject" labels.

---

### 3.5 Stage 4: Full Agentic Distillation

**Objective:** Distill from a long-context teacher LLM performing real agentic tasks.

#### 3.5.1 Teacher-Student Setup

*   **Teacher ($T$):** Long-context SOTA model (e.g., 128k+ context) with full repository access in prompt.
*   **Student ($S$):** Awareness model with repository in latent memory only.

#### 3.5.2 Task Distribution

*   Real GitHub issues → resolution traces
*   Multi-file refactoring tasks
*   Feature implementation from natural language specs
*   Bug reproduction and fixing

#### 3.5.3 Distillation Variants

1.  **Full Trace Distillation:**
    *   Teacher performs complete task (reads, plans, edits).
    *   Student learns to replicate entire trace.

2.  **Filtered Trace Distillation:**
    *   Remove teacher file reads that don't relate to final edits.
    *   Student learns efficient attention patterns.

3.  **Output-Only Distillation:**
    *   Teacher produces final code output.
    *   Student must independently navigate to same result.
    *   Harder but encourages genuine understanding.

#### 3.5.4 Loss Functions

1.  **KL Divergence:** Match teacher's token distribution.
2.  **Behavioral Cloning:** Match teacher's tool-use decisions (which files to read, edit order).
3.  **Sparsity Regularization:** Penalize encoder for redundant KV pairs—encourage compression.

---

### 3.6 Joint Encoder-Decoder Training

#### 3.6.1 Three-Phase Freeze Schedule

Training uses a progressive unfreezing strategy inspired by InstructRETRO's finding that unfreezing the decoder during retrieval-augmented training yields substantially better results than keeping it frozen throughout.

**Phase A — GCA Bootstrap (Stage 0, first ~30% of steps):**
*   Decoder base: **frozen**.
*   Trainable: GCA blocks (lr=1e-4), encoder (lr=1e-5).
*   Goal: GCA gates grow from ~0.27 toward ~0.5+, projections learn useful attention patterns. The frozen decoder provides a stable target for the encoder and GCA to calibrate against.
*   Transition criterion: average gate value > 0.4 **and** eval accuracy clearly above baseline (no-memory).

**Phase B — Decoder Adaptation (Stage 0 remainder + Stage 1):**
*   Decoder base: **unfrozen** at very low lr (1e-6, ~10× below GCA lr).
*   Trainable: all parameters. Decoder base uses its own param group.
*   Goal: decoder internal representations shift to better exploit cross-attention signal. InstructRETRO showed this is where the largest quality gains come from — the decoder learns to "expect" external memory, not just tolerate it.
*   Cosine schedule for all param groups from this point forward.

**Phase C — Full Joint Training (Stages 2–4):**
*   Decoder base: **unfrozen** at moderate lr (5e-6).
*   Encoder may be partially frozen or use LoRA to prevent drift on complex data.
*   Goal: all components co-adapt on realistic tasks.

#### 3.6.2 Invariants Across All Phases

*   Gradients flow from decoder loss through cross-attention into encoder, forcing useful representations.
*   Exploit temporal locality: when training on commit sequences, only re-encode changed files.
*   Monitor for catastrophic forgetting: periodically evaluate the decoder on a held-out language modeling benchmark (e.g., a slice of C4) to ensure base capabilities are preserved. If perplexity degrades >10%, reduce decoder lr or re-freeze temporarily.

---

## 4. Target Application: The "Infinite" Coding Agent

The primary implementation target is a coding agent with **Full-Repo Awareness**.

### 4.1 The Bottleneck
Current coding assistants rely on RAG (imprecise) or stuffing context (expensive/limited). They fail at:
*   System-wide refactors.
*   Understanding distant dependencies.
*   Maintaining awareness of project structure updates.

### 4.2 The Awareness Solution
*   **Initialization:** The entire repository is passed through the Context Encoder once.
*   **Inference:** The user asks to "fix the bug in `auth.py`." The Decoder attends to `auth.py` (local context) and Cross-Attends to the entire repo's KV store (latent memory).
*   **Mutation:** The agent writes code. The modified file is re-encoded in milliseconds. The memory store is updated. The agent immediately "sees" the change in its latent space.

---

## 5. Implementation Plan

### 5.1 Model Selection

The Qwen3 series provides an ideal progression path from rapid prototyping to production deployment. See [QWEN3_IMPLEMENTATION.md](QWEN3_IMPLEMENTATION.md) for detailed implementation specifics.

#### 5.1.1 Decoder ($D_\phi$) — Progression Path

| Phase | Model | Parameters | Active Params | Hardware | Purpose |
|-------|-------|------------|---------------|----------|---------|
| **Proto-1** | Qwen3-0.6B | 0.6B | 0.6B | Single RTX 3060 | Architecture validation, fast iteration |
| **Proto-2** | Qwen3-1.7B | 1.7B | 1.7B | Single RTX 3060 | Cross-attention integration testing |
| **Dev** | Qwen3-4B | 4B | 4B | RTX 4090 / A100-40G | Training pipeline validation |
| **Scale** | Qwen3-8B | 8B | 8B | A100-40G | Full training runs |
| **Prod** | Qwen3-30B-A3B | 30.5B | 3.3B | 2× RTX 4090 | Production (MoE efficiency) |
| **Prod-Alt** | Qwen3-14B | 14.8B | 14.8B | A100-80G | Production (dense alternative) |

**Key Architecture Properties:**
*   **Grouped Query Attention (GQA):** 4:1 ratio (query:KV heads) — reduces memory, compatible with cross-attention injection
*   **RoPE:** Base frequency 1M, extendable to 131K tokens via YaRN
*   **Native Context:** 32K tokens (sufficient for local prompt + scratchpad)

#### 5.1.2 Encoder ($E_\theta$) — Qwen3 Embedding Series

| Model | Parameters | Embedding Dim | Context | MTEB Score |
|-------|------------|---------------|---------|------------|
| **Qwen3-Embedding-0.6B** | 0.6B | 1,024 | 32K | — |
| **Qwen3-Embedding-4B** | 4B | 2,560 | 32K | — |
| **Qwen3-Embedding-8B** | 8B | 4,096 | 32K | 70.58 (#1) |

**Why Qwen3-Embedding:**
*   **Architecture Match:** Same tokenizer, compatible hidden dimensions for KV projection
*   **Bidirectional:** Uses full self-attention (not causal), ideal for document encoding
*   **Pre-trained for Retrieval:** Already optimized for semantic representation
*   **MRL Support:** Matryoshka Representation Learning allows flexible embedding dimensions

**Encoder-Decoder Dimension Alignment:**

| Encoder | Encoder Hidden | Decoder | Decoder Hidden | Projection |
|---------|----------------|---------|----------------|------------|
| Embedding-0.6B | 1,024 | Qwen3-0.6B | ~1,024 | None needed |
| Embedding-4B | 2,560 | Qwen3-4B | 2,560 | None needed |
| Embedding-8B | 4,096 | Qwen3-8B | 4,096 | None needed |

The dimension alignment is intentional — Qwen3 Embedding models were derived from corresponding decoder models.

### 5.2 Dataset Ideas

1.  **The "RepoStack":** GitHub repositories.
2.  **Distilled Traces:**
    *   Take existing commits and reverse-encode them as tasks prompts using an LLM.
    *   Distill larger Qwen3 model teacher creating the commits.
3.  **Negative Sampling:** Training the model *not* to attend to irrelevant files (anti-hallucination).

---

## 6. Significance

**Awareness** represents a shift from **stateless** to **stateful** LLM architectures. By treating context as a database of pre-computed tensors rather than a sequence of raw tokens, we achieve:

1.  **Context Decoupling:** Context size is limited only by disk/RAM, not VRAM or $O(N^2)$ attention walls.
2.  **Persistence:** Knowledge bases do not need to be re-digested for every query.
3.  **Privacy/Security:** Memory stores can be physically separated or permission-gated at the tensor retrieval level without retraining the core model.
